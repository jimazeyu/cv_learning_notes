{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "from fvcore.nn import FlopCountAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# CIFAR-10 数据集的转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 调整图片大小以匹配 ViT 输入\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# 加载 CIFAR-10 数据集\n",
    "train_dataset = datasets.CIFAR10(root='~/.cache', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='~/.cache', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels, embed_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.projection = nn.Conv2d(in_channels, embed_size, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)  # [B, C, H, W] -> [B, E, H', W']\n",
    "        x = x.flatten(2)        # [B, E, H', W'] -> [B, E, N]\n",
    "        x = x.transpose(1, 2)   # [B, E, N] -> [B, N, E]\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=32, in_channels=3, embed_size=768, num_heads=12, num_layers=12, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = PatchEmbedding(img_size, patch_size, in_channels, embed_size)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_size))\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, 1 + (img_size // patch_size) ** 2, embed_size))\n",
    "\n",
    "        encoder_layers = nn.TransformerEncoderLayer(embed_size, num_heads, dim_feedforward=embed_size * 4)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "\n",
    "        self.to_cls_token = nn.Identity()\n",
    "        self.fc = nn.Linear(embed_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.patch_embedding(x)\n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        x += self.positional_encoding\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.to_cls_token(x[:, 0])\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jimaz\\anaconda3\\envs\\python38\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT(\n",
      "  (patch_embedding): PatchEmbedding(\n",
      "    (projection): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (to_cls_token): Identity()\n",
      "  (fc): Linear(in_features=768, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ViT().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "├─PatchEmbedding: 1-1                         [-1, 49, 768]             --\n",
      "|    └─Conv2d: 2-1                            [-1, 768, 7, 7]           2,360,064\n",
      "├─TransformerEncoder: 1-2                     [-1, 50, 768]             --\n",
      "|    └─ModuleList: 2                          []                        --\n",
      "|    |    └─TransformerEncoderLayer: 3-1      [-1, 50, 768]             7,087,872\n",
      "|    |    └─TransformerEncoderLayer: 3-2      [-1, 50, 768]             7,087,872\n",
      "|    |    └─TransformerEncoderLayer: 3-3      [-1, 50, 768]             7,087,872\n",
      "|    |    └─TransformerEncoderLayer: 3-4      [-1, 50, 768]             7,087,872\n",
      "|    |    └─TransformerEncoderLayer: 3-5      [-1, 50, 768]             7,087,872\n",
      "|    |    └─TransformerEncoderLayer: 3-6      [-1, 50, 768]             7,087,872\n",
      "|    |    └─TransformerEncoderLayer: 3-7      [-1, 50, 768]             7,087,872\n",
      "|    |    └─TransformerEncoderLayer: 3-8      [-1, 50, 768]             7,087,872\n",
      "|    |    └─TransformerEncoderLayer: 3-9      [-1, 50, 768]             7,087,872\n",
      "|    |    └─TransformerEncoderLayer: 3-10     [-1, 50, 768]             7,087,872\n",
      "|    |    └─TransformerEncoderLayer: 3-11     [-1, 50, 768]             7,087,872\n",
      "|    |    └─TransformerEncoderLayer: 3-12     [-1, 50, 768]             7,087,872\n",
      "├─Identity: 1-3                               [-1, 768]                 --\n",
      "├─Linear: 1-4                                 [-1, 10]                  7,690\n",
      "===============================================================================================\n",
      "Total params: 87,422,218\n",
      "Trainable params: 87,422,218\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 372.83\n",
      "===============================================================================================\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 24.90\n",
      "Params size (MB): 333.49\n",
      "Estimated Total Size (MB): 358.96\n",
      "===============================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add_ encountered 1 time(s)\n",
      "Unsupported operator aten::div encountered 12 time(s)\n",
      "Unsupported operator aten::unflatten encountered 12 time(s)\n",
      "Unsupported operator aten::mul encountered 48 time(s)\n",
      "Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "Unsupported operator aten::add encountered 24 time(s)\n",
      "The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "transformer_encoder.layers.0.self_attn.out_proj, transformer_encoder.layers.1.self_attn.out_proj, transformer_encoder.layers.10.self_attn.out_proj, transformer_encoder.layers.11.self_attn.out_proj, transformer_encoder.layers.2.self_attn.out_proj, transformer_encoder.layers.3.self_attn.out_proj, transformer_encoder.layers.4.self_attn.out_proj, transformer_encoder.layers.5.self_attn.out_proj, transformer_encoder.layers.6.self_attn.out_proj, transformer_encoder.layers.7.self_attn.out_proj, transformer_encoder.layers.8.self_attn.out_proj, transformer_encoder.layers.9.self_attn.out_proj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPs: 4.37GFLOPS\n"
     ]
    }
   ],
   "source": [
    "summary(model, (3, 224, 224))\n",
    "flops = FlopCountAnalysis(model, torch.randn(1, 3, 224, 224).to(device))\n",
    "print(\"FLOPs: {:.2f}GFLOPS\".format(flops.total() / 1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  22%|██▏       | 171/782 [00:32<01:55,  5.30batch/s, loss=2.36]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     17\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 19\u001b[0m         total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m         tepoch\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     22\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练循环\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", unit='batch') as tepoch:\n",
    "        for images, labels in tepoch:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 模型评估\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
