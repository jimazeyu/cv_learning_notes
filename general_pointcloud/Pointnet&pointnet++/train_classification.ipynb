{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876bf190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils.ModelNetDataLoader import ModelNetDataLoader\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "# 设置基本的目录变量\n",
    "BASE_DIR = os.path.dirname(os.path.abspath('__file__')) # 基本目录\n",
    "ROOT_DIR = BASE_DIR # 根目录\n",
    "sys.path.append(os.path.join(ROOT_DIR, 'models')) # 模型目录\n",
    "\n",
    "# 显示当前目录\n",
    "print(f\"当前工作目录：{BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef664b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_string(str):\n",
    "        logger.info(str)\n",
    "        print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9776c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    '''PARAMETERS'''\n",
    "    parser = argparse.ArgumentParser('training')\n",
    "    parser.add_argument('--use_cpu', action='store_true', default=False, help='use cpu mode')\n",
    "    parser.add_argument('--gpu', type=str, default='0', help='specify gpu device')\n",
    "    parser.add_argument('--batch_size', type=int, default=24, help='batch size in training')\n",
    "    parser.add_argument('--model', default='pointnet_cls', help='model name [default: pointnet_cls]')\n",
    "    parser.add_argument('--num_category', default=40, type=int, choices=[10, 40],  help='training on ModelNet10/40')\n",
    "    parser.add_argument('--epoch', default=200, type=int, help='number of epoch in training')\n",
    "    parser.add_argument('--learning_rate', default=0.001, type=float, help='learning rate in training')\n",
    "    parser.add_argument('--num_point', type=int, default=1024, help='Point Number')\n",
    "    parser.add_argument('--optimizer', type=str, default='Adam', help='optimizer for training')\n",
    "    parser.add_argument('--log_dir', type=str, default=None, help='experiment root')\n",
    "    parser.add_argument('--decay_rate', type=float, default=1e-4, help='decay rate')\n",
    "    parser.add_argument('--use_normals', action='store_true', default=False, help='use normals')\n",
    "    parser.add_argument('--process_data', action='store_true', default=False, help='save data offline')\n",
    "    parser.add_argument('--use_uniform_sample', action='store_true', default=False, help='use uniform sampiling')\n",
    "    parser.add_argument('--continue_from_checkpoint', action='store_true', help='Continue training from checkpoint if available')\n",
    "    parser.add_argument('--no_continue_from_checkpoint', action='store_false', dest='continue_from_checkpoint', help='Start training from scratch')\n",
    "    parser.set_defaults(continue_from_checkpoint=True)\n",
    "    return parser.parse_args()\n",
    "    \n",
    "# 测试参数解析函数\n",
    "args = parse_args()\n",
    "print(f\"解析到的参数：{args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2b3da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''HYPER PARAMETER'''\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "\n",
    "'''CREATE DIR'''\n",
    "timestr = str(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M'))\n",
    "exp_dir = Path('./log/')\n",
    "exp_dir.mkdir(exist_ok=True)\n",
    "exp_dir = exp_dir.joinpath('classification')\n",
    "exp_dir.mkdir(exist_ok=True)\n",
    "if args.log_dir is None:\n",
    "    exp_dir = exp_dir.joinpath(timestr)\n",
    "else:\n",
    "    exp_dir = exp_dir.joinpath(args.log_dir)\n",
    "exp_dir.mkdir(exist_ok=True)\n",
    "checkpoints_dir = exp_dir.joinpath('checkpoints/')\n",
    "checkpoints_dir.mkdir(exist_ok=True)\n",
    "log_dir = exp_dir.joinpath('logs/')\n",
    "log_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eec272",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be11f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个inplace_relu函数。这个函数将被应用到模型的每个模块（或子模块）上。\n",
    "# 如果模块是ReLU（Rectified Linear Unit）激活函数，这个函数会设置其为原地操作（in-place），这意味着直接修改输入而不是创建一个新的输出。\n",
    "def inplace_relu(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('ReLU') != -1:\n",
    "        m.inplace=True\n",
    "\n",
    "# 测试 inplace_relu 函数\n",
    "print(\"测试 inplace_relu 函数...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5192ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 定义一个测试函数，用于评估模型在给定数据集上的性能。\n",
    "# model: 要评估的模型\n",
    "# loader: 数据加载器，包含测试数据\n",
    "# num_class: 类别的数量，默认为40（ModelNet40数据集）\n",
    "def test(model, loader, num_class=40):\n",
    "    mean_correct = []  # 用于存储每个批次的平均正确率\n",
    "    class_acc = np.zeros((num_class, 3))  # 用于存储每个类别的准确率\n",
    "    classifier = model.eval()  # 将模型设置为评估模式\n",
    "\n",
    "    # 对加载器中的每个批次进行迭代\n",
    "    for j, (points, target) in tqdm(enumerate(loader), total=len(loader)):\n",
    "\n",
    "        # 如果不使用CPU，则将数据移动到GPU\n",
    "        if not args.use_cpu:\n",
    "            points, target = points.cuda(), target.cuda()\n",
    "\n",
    "        # 调整点云数据的维度\n",
    "        points = points.transpose(2, 1)\n",
    "        # 通过模型进行预测\n",
    "        pred, _ = classifier(points)\n",
    "        # 获取每个点云的预测类别\n",
    "        pred_choice = pred.data.max(1)[1]\n",
    "\n",
    "        # 输出预测和实际的类别，以便观察模型的表现\n",
    "        print(f\"Batch {j+1}:\")\n",
    "        print(\"Predicted classes:\", pred_choice)\n",
    "        print(\"Actual classes:\", target)\n",
    "\n",
    "\n",
    "        # 对每个独特类别的点云进行迭代\n",
    "        for cat in np.unique(target.cpu()):\n",
    "            # 计算每个类别的准确率\n",
    "            classacc = pred_choice[target == cat].eq(target[target == cat].long().data).cpu().sum()\n",
    "            class_acc[cat, 0] += classacc.item() / float(points[target == cat].size()[0])\n",
    "            class_acc[cat, 1] += 1\n",
    "\n",
    "        # 计算整个批次的正确率\n",
    "        correct = pred_choice.eq(target.long().data).cpu().sum()\n",
    "        mean_correct.append(correct.item() / float(points.size()[0]))\n",
    "\n",
    "    # 计算每个类别的平均准确率\n",
    "    class_acc[:, 2] = class_acc[:, 0] / class_acc[:, 1]\n",
    "    class_acc = np.mean(class_acc[:, 2])\n",
    "    # 计算所有批次的平均正确率\n",
    "    instance_acc = np.mean(mean_correct)\n",
    "\n",
    "    # 在函数结束时输出最终的分类准确率\n",
    "    print(f\"Instance Accuracy: {instance_acc}\")\n",
    "    print(f\"Class Accuracy: {class_acc}\")\n",
    "    \n",
    "    return instance_acc, class_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c6d394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    def log_string(str):\n",
    "        logger.info(str)\n",
    "        print(str)\n",
    "\n",
    "    '''HYPER PARAMETER'''\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
    "\n",
    "    '''CREATE DIR'''\n",
    "    timestr = str(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M'))\n",
    "    exp_dir = Path('./log/')\n",
    "    exp_dir.mkdir(exist_ok=True)\n",
    "    exp_dir = exp_dir.joinpath('classification')\n",
    "    exp_dir.mkdir(exist_ok=True)\n",
    "    if args.log_dir is None:\n",
    "        exp_dir = exp_dir.joinpath(timestr)\n",
    "    else:\n",
    "        exp_dir = exp_dir.joinpath(args.log_dir)\n",
    "    exp_dir.mkdir(exist_ok=True)\n",
    "    checkpoints_dir = exp_dir.joinpath('checkpoints/')\n",
    "    checkpoints_dir.mkdir(exist_ok=True)\n",
    "    log_dir = exp_dir.joinpath('logs/')\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # 解析命令行参数\n",
    "    # 创建日志记录器\n",
    "    '''LOG'''\n",
    "    args = parse_args()\n",
    "    logger = logging.getLogger(\"Model\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    file_handler = logging.FileHandler('%s/%s.txt' % (log_dir, args.model))\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    log_string('PARAMETER ...')\n",
    "    log_string(args)\n",
    "\n",
    "    test_instance_acc_history = []\n",
    "    test_class_acc_history = []\n",
    "\n",
    "    '''DATA LOADING'''\n",
    "    log_string('Load dataset ...')\n",
    "    data_path = 'data/modelnet40_normal_resampled/'\n",
    "\n",
    "    train_dataset = ModelNetDataLoader(root=data_path, args=args, split='train', process_data=args.process_data)\n",
    "    test_dataset = ModelNetDataLoader(root=data_path, args=args, split='test', process_data=args.process_data)\n",
    "    trainDataLoader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=10, drop_last=True)\n",
    "    testDataLoader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=10)\n",
    "\n",
    "    '''MODEL LOADING'''\n",
    "    num_class = args.num_category\n",
    "    model = importlib.import_module(args.model)\n",
    "    shutil.copy('./models/%s.py' % args.model, str(exp_dir))\n",
    "    shutil.copy('models/pointnet2_utils.py', str(exp_dir))\n",
    "    shutil.copy('./train_classification.py', str(exp_dir))\n",
    "\n",
    "    classifier = model.get_model(num_class, normal_channel=args.use_normals)\n",
    "    criterion = model.get_loss()\n",
    "    classifier.apply(inplace_relu)\n",
    "\n",
    "    if not args.use_cpu:\n",
    "        classifier = classifier.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    if args.continue_from_checkpoint:\n",
    "        try:\n",
    "            checkpoint = torch.load(str(exp_dir) + '/checkpoints/best_model.pth')\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            classifier.load_state_dict(checkpoint['model_state_dict'])\n",
    "            log_string('Use pretrain model')\n",
    "        except:\n",
    "            log_string('No existing model, starting training from scratch...')\n",
    "            start_epoch = 0\n",
    "    else:\n",
    "        log_string('Starting training from scratch...')\n",
    "        start_epoch = 0\n",
    "\n",
    "\n",
    "    if args.optimizer == 'Adam':\n",
    "        optimizer = torch.optim.Adam(\n",
    "            classifier.parameters(),\n",
    "            lr=args.learning_rate,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-08,\n",
    "            weight_decay=args.decay_rate\n",
    "        )\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(classifier.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.7)\n",
    "    global_epoch = 0\n",
    "    global_step = 0\n",
    "    best_instance_acc = 0.0\n",
    "    best_class_acc = 0.0\n",
    "\n",
    "    '''TRANING'''\n",
    "    logger.info('Start training...')\n",
    "    for epoch in range(start_epoch, args.epoch):\n",
    "        #训练过程\n",
    "        log_string('Epoch %d (%d/%s):' % (global_epoch + 1, epoch + 1, args.epoch))\n",
    "        mean_correct = []\n",
    "        classifier = classifier.train()\n",
    "\n",
    "        scheduler.step()\n",
    "        for batch_id, (points, target) in tqdm(enumerate(trainDataLoader, 0), total=len(trainDataLoader), smoothing=0.9):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            points = points.data.numpy()\n",
    "            points = provider.random_point_dropout(points)\n",
    "            points[:, :, 0:3] = provider.random_scale_point_cloud(points[:, :, 0:3])\n",
    "            points[:, :, 0:3] = provider.shift_point_cloud(points[:, :, 0:3])\n",
    "            points = torch.Tensor(points)\n",
    "            points = points.transpose(2, 1)\n",
    "\n",
    "            if not args.use_cpu:\n",
    "                points, target = points.cuda(), target.cuda()\n",
    "\n",
    "            pred, trans_feat = classifier(points)\n",
    "            loss = criterion(pred, target.long(), trans_feat)\n",
    "            pred_choice = pred.data.max(1)[1]\n",
    "\n",
    "            correct = pred_choice.eq(target.long().data).cpu().sum()\n",
    "            mean_correct.append(correct.item() / float(points.size()[0]))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "\n",
    "        train_instance_acc = np.mean(mean_correct)\n",
    "        log_string('Train Instance Accuracy: %f' % train_instance_acc)\n",
    "\n",
    "        #测试过程\n",
    "        with torch.no_grad():\n",
    "            instance_acc, class_acc = test(classifier.eval(), testDataLoader, num_class=num_class)\n",
    "\n",
    "            # 将测试精度添加到列表中\n",
    "            test_instance_acc_history.append(instance_acc)\n",
    "            test_class_acc_history.append(class_acc)\n",
    "\n",
    "            if (instance_acc >= best_instance_acc):\n",
    "                best_instance_acc = instance_acc\n",
    "                best_epoch = epoch + 1\n",
    "\n",
    "            if (class_acc >= best_class_acc):\n",
    "                best_class_acc = class_acc\n",
    "            log_string('Test Instance Accuracy: %f, Class Accuracy: %f' % (instance_acc, class_acc))\n",
    "            log_string('Best Instance Accuracy: %f, Class Accuracy: %f' % (best_instance_acc, best_class_acc))\n",
    "\n",
    "            if (instance_acc >= best_instance_acc):\n",
    "                logger.info('Save model...')\n",
    "                savepath = str(checkpoints_dir) + '/best_model.pth'\n",
    "                log_string('Saving at %s' % savepath)\n",
    "                state = {\n",
    "                    'epoch': best_epoch,\n",
    "                    'instance_acc': instance_acc,\n",
    "                    'class_acc': class_acc,\n",
    "                    'model_state_dict': classifier.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }\n",
    "                torch.save(state, savepath)\n",
    "            global_epoch += 1\n",
    "\n",
    "    logger.info('End of training...')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parse_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6a3c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数解析\n",
    "args = parse_args()\n",
    "print(\"解析到的参数：\", args)\n",
    "\n",
    "# 创建日志记录器\n",
    "logger = logging.getLogger(\"Model\")\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler = logging.FileHandler('%s/%s.txt' % (log_dir, args.model))\n",
    "file_handler.setLevel(logging.INFO)\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "log_string('参数设置：')\n",
    "log_string(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eb4ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instance_acc_history = []\n",
    "test_class_acc_history = []\n",
    "# 加载数据集\n",
    "log_string('Load dataset ...')\n",
    "data_path = 'data/modelnet40_normal_resampled/'\n",
    "\n",
    "train_dataset = ModelNetDataLoader(root=data_path, args=args, split='train', process_data=args.process_data)\n",
    "test_dataset = ModelNetDataLoader(root=data_path, args=args, split='test', process_data=args.process_data)\n",
    "trainDataLoader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=10, drop_last=True)\n",
    "testDataLoader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=10)\n",
    "\n",
    "# 输出数据集信息\n",
    "print(\"训练集大小：\", len(train_dataset))\n",
    "print(\"测试集大小：\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a9a79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 模型初始化\n",
    "log_string('正在初始化模型...')\n",
    "'''MODEL LOADING'''\n",
    "num_class = args.num_category\n",
    "model = importlib.import_module(args.model)\n",
    "shutil.copy('./models/%s.py' % args.model, str(exp_dir))\n",
    "shutil.copy('models/pointnet2_utils.py', str(exp_dir))\n",
    "shutil.copy('./train_classification.py', str(exp_dir))\n",
    "\n",
    "classifier = model.get_model(num_class, normal_channel=args.use_normals)\n",
    "criterion = model.get_loss()\n",
    "classifier.apply(inplace_relu)\n",
    "\n",
    "if not args.use_cpu:\n",
    "    classifier = classifier.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "if args.continue_from_checkpoint:\n",
    "    try:\n",
    "        checkpoint = torch.load(str(exp_dir) + '/checkpoints/best_model.pth')\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        classifier.load_state_dict(checkpoint['model_state_dict'])\n",
    "        log_string('Use pretrain model')\n",
    "    except:\n",
    "        log_string('No existing model, starting training from scratch...')\n",
    "        start_epoch = 0\n",
    "else:\n",
    "    log_string('Starting training from scratch...')\n",
    "    start_epoch = 0\n",
    "\n",
    "\n",
    "if args.optimizer == 'Adam':\n",
    "    optimizer = torch.optim.Adam(\n",
    "        classifier.parameters(),\n",
    "        lr=args.learning_rate,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-08,\n",
    "        weight_decay=args.decay_rate\n",
    "    )\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(classifier.parameters(), lr=0.01, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3354d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "logger.info('开始训练...')\n",
    "for epoch in range(start_epoch, args.epoch):\n",
    "        #训练过程\n",
    "        log_string('Epoch %d (%d/%s):' % (global_epoch + 1, epoch + 1, args.epoch))\n",
    "        mean_correct = []\n",
    "        classifier = classifier.train()\n",
    "\n",
    "        scheduler.step()\n",
    "        for batch_id, (points, target) in tqdm(enumerate(trainDataLoader, 0), total=len(trainDataLoader), smoothing=0.9):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            points = points.data.numpy()\n",
    "            points = provider.random_point_dropout(points)\n",
    "            points[:, :, 0:3] = provider.random_scale_point_cloud(points[:, :, 0:3])\n",
    "            points[:, :, 0:3] = provider.shift_point_cloud(points[:, :, 0:3])\n",
    "            points = torch.Tensor(points)\n",
    "            points = points.transpose(2, 1)\n",
    "\n",
    "            if not args.use_cpu:\n",
    "                points, target = points.cuda(), target.cuda()\n",
    "\n",
    "            pred, trans_feat = classifier(points)\n",
    "            loss = criterion(pred, target.long(), trans_feat)\n",
    "            pred_choice = pred.data.max(1)[1]\n",
    "\n",
    "            correct = pred_choice.eq(target.long().data).cpu().sum()\n",
    "            mean_correct.append(correct.item() / float(points.size()[0]))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            print(f\"批次 {batch_id+1}/{len(trainDataLoader)}，损失：{loss.item()}\")\n",
    "\n",
    "        train_instance_acc = np.mean(mean_correct)\n",
    "        log_string('Train Instance Accuracy: %f' % train_instance_acc)\n",
    "        # 输出训练轮次的结果\n",
    "        print(f\"训练完成，第 {epoch+1}/{args.epoch} 轮，准确率：{train_instance_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c27bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试过程\n",
    "with torch.no_grad():\n",
    "    instance_acc, class_acc = test(classifier.eval(), testDataLoader, num_class=num_class)\n",
    "\n",
    "    # 将测试精度添加到列表中\n",
    "    test_instance_acc_history.append(instance_acc)\n",
    "    test_class_acc_history.append(class_acc)\n",
    "\n",
    "    # 输出测试结果\n",
    "    log_string('测试实例准确率: %f, 类别准确率: %f' % (instance_acc, class_acc))\n",
    "    log_string('最佳实例准确率: %f, 类别准确率: %f' % (best_instance_acc, best_class_acc))\n",
    "\n",
    "    # 保存最佳模型\n",
    "    if (instance_acc >= best_instance_acc):\n",
    "        # 保存操作\n",
    "        log_string('保存模型中...')\n",
    "        best_instance_acc = instance_acc\n",
    "        best_epoch = epoch + 1\n",
    "\n",
    "    if (class_acc >= best_class_acc):\n",
    "        best_class_acc = class_acc\n",
    "    log_string('Test Instance Accuracy: %f, Class Accuracy: %f' % (instance_acc, class_acc))\n",
    "    log_string('Best Instance Accuracy: %f, Class Accuracy: %f' % (best_instance_acc, best_class_acc))\n",
    "\n",
    "    if (instance_acc >= best_instance_acc):\n",
    "        logger.info('Save model...')\n",
    "        savepath = str(checkpoints_dir) + '/best_model.pth'\n",
    "        log_string('Saving at %s' % savepath)\n",
    "        state = {\n",
    "            'epoch': best_epoch,\n",
    "            'instance_acc': instance_acc,\n",
    "            'class_acc': class_acc,\n",
    "            'model_state_dict': classifier.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }\n",
    "        torch.save(state, savepath)\n",
    "    global_epoch += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154f27ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查是否有图形界面环境（例如：使用DISPLAY环境变量）\n",
    "if os.environ.get('DISPLAY', None):\n",
    "    # 绘图\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(test_instance_acc_history, label='Test Instance Accuracy')\n",
    "    plt.title('Test Instance Accuracy Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(test_class_acc_history, label='Class Accuracy')\n",
    "    plt.title('Class Accuracy Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "else:\n",
    "    # 没有图形界面环境，将图表保存为文件\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(test_instance_acc_history, label='Test Instance Accuracy')\n",
    "    plt.title('Test Instance Accuracy Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(test_class_acc_history, label='Class Accuracy')\n",
    "    plt.title('Class Accuracy Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "    plt.savefig('accuracy_plot.png')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
